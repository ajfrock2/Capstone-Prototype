# Results Log — AI Benchmark Tests

## Instructions

Record one row per test run. Use the rubric (`RUBRIC.md`) for scoring. Keep entries consistent for fair comparison.

---

## Log Format

| Test ID | Date | AI Tool | AI Model/Version | Benchmark Task | Time to Working (min) | Iterations to Fix | Code Quality /20 | Unity Practices /20 | Maintainability /20 | Arch. Consistency /15 | Error Risk /15 | Intervention /10 | Total /100 | Unity-Specific Issues | Notable Patterns | Files Generated |
|---------|------|---------|------------------|----------------|------------------------|-------------------|------------------|---------------------|---------------------|------------------------|----------------|------------------|------------|----------------------|------------------|-----------------|
| FL-001 | | *(User: run with first AI tool)* | | Flashlight System | | | | | | | | | | | | |

---

## Test Run Details (Expand per run)

### FL-001 — [AI Tool] — [Date]

**Interview Q&A:**
- 

**Approved Plan Summary:**
- 

**Integration Notes:**
- 

**Issues Found:**
- 

**Rubric Notes:**
- Code Quality: 
- Unity Practices: 
- Maintainability: 
- Arch. Consistency: 
- Error Risk: 
- Intervention: 

---

## CSV Export Format

For spreadsheet analysis, use this header row:

```
TestID,Date,AITool,AIModelVersion,BenchmarkTask,TimeToWorkingMin,IterationsToFix,CodeQuality,UnityPractices,Maintainability,ArchConsistency,ErrorRisk,Intervention,Total,UnitySpecificIssues,NotablePatterns,FilesGenerated
```
